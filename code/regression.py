# -*- coding: utf-8 -*-
"""[Regression task, final] bayes-by-backprop-regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v2ot0w96eFKQ6GNDswNsmqEiIAwbquZU
"""

# %matplotlib inline
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import math
import numpy as np
import matplotlib.pyplot as plt
!pip install namedlist
from namedlist import namedlist

class MLP(nn.Module):
    def __init__(self, in_units, hidden_units, out_units):
        super().__init__()
        self.in_units = in_units
        self.fc1 = nn.Linear(in_units, hidden_units)
        self.fc2 = nn.Linear(hidden_units, hidden_units)
        self.fc3 = nn.Linear(hidden_units, out_units)

    def forward(self, x):
        x = x.view(-1, self.in_units)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class Dropout_MLP(nn.Module):
    '''MLP with dropout on both hidden layers
       p=0.5, as per the original paper
    '''
    def __init__(self, in_units, hidden_units, out_units, prob):
        super().__init__()
        self.in_units = in_units
        self.prob = prob
        self.fc1 = nn.Linear(in_units, hidden_units)
        self.fc2 = nn.Linear(hidden_units, hidden_units)
        self.fc3 = nn.Linear(hidden_units, out_units)
#         self.drop1 = nn.Dropout(p=0.5)
#         self.drop2 = nn.Dropout(p=0.5)
#         self.drop3 = nn.Dropout(p=0.5)

    def forward(self, x):
        x = x.view(-1, self.in_units)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, self.prob, True, False)
        x = F.relu(self.fc2(x))
        x = F.dropout(x, self.prob, True, False)
        x = self.fc3(x)
        return x

def train(model, data, target, optimizer, loss_fn, device='cpu'):
    optimizer.zero_grad()
    if isinstance(model, MLP) or isinstance(model, Dropout_MLP): # ugly code coming up
        output = model(data)
        assert isinstance(loss_fn, nn.MSELoss)
        loss = loss_fn(output, target)
        print(loss)
    elif isinstance(model, BayesianNet):
        assert loss_fn == model.loss
        loss = loss_fn(data, target, batch_idx=None, device=device)
    else:
        print(loss_fn)
        assert False
    loss.backward()
    optimizer.step()
    return model

"""Regression curves
$$y = x + 0.3 \sin (2\pi(x+\epsilon)) + 0.3\sin(4\pi(x+\epsilon)) + \epsilon$$
$$\epsilon \sim N(0,0.02^2)$$
"""

# For reproducability
torch.manual_seed(0)
if torch.cuda.is_available():
  torch.backends.cudnn.deterministic = True
  torch.backends.cudnn.benchmark = False
np.random.seed(0)

# Load device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
###
#device = torch.device("cpu")
###
loader_kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}
print('Using device: {}'.format(device))

# Load data
num_points = 5000
mean = 0
std = 0.02 # N(0,0.02) typically means variance = 0.02, but I think they mean std
x_data = np.random.rand(num_points) * 0.5 # points drawn u.a.r. from [0,0.5)
epsilon = np.random.normal(mean, std, num_points)
y_data = x_data + 0.3*np.sin( 2*np.pi*(x_data + epsilon) ) + \
                    + 0.3*np.sin( 4*np.pi*(x_data + epsilon) ) + \
                    + epsilon
# y_data = x_data**3 + epsilon
x_all = np.linspace(-0.4, 1.2, 1600)

# convert data to torch tensors and unsqueeze (we need data to have dim 500x1 not just 500)
x_data = torch.unsqueeze(torch.from_numpy(x_data).float(), dim=1)
y_data = torch.unsqueeze(torch.from_numpy(y_data).float(), dim=1)
x_all = torch.unsqueeze(torch.from_numpy(x_all).float(), dim=1)

# put on GPU for BayesianNet
if device == torch.device('cuda'):
    x_cuda = x_data.to(device)
    y_cuda = y_data.to(device)
    x_all_cuda = x_all.to(device)

# Set up experiment
Args = namedlist('Args', ['epochs', 'batch_size', 'test_batch_size',
                          'num_batches_train',
                          'num_classes', 'device', 'n_test_samples',
                          ('log_interval', None),
                          # last two are only for BayesianNet
                          ('loader_kwargs', None),
                          ('valid_set_frac', None)])
args = Args(epochs=450,
            batch_size=num_points, # note 9
            test_batch_size=num_points, # note 10
            num_batches_train = 1,
            num_classes=1, # output dimensionality
            device=device,
            n_test_samples=1000
           )

y_MLP = []
for num in range(10):
  mlp = MLP(1, 800, 1)
  optimizer = optim.Adam(mlp.parameters())
  squared_loss = nn.MSELoss()

  mlp.train()
  for epoch in range(1, args.epochs + 1):
      mlp = train(mlp, x_data, y_data, optimizer, squared_loss)
      if epoch % 50 == 0:
          print('Training MLP, epoch {}'.format(epoch))

  n_test_samples = 1000
  mlp.eval()
  with torch.no_grad():
    print(num)
    y_MLP.append(mlp(x_all))

p = 0.5
bayesian = False
dropout_mlp = Dropout_MLP(1, 800, 1, p)
if bayesian:
  var = 1
  sigma = 1e-2
  w_decay = sigma ** 2 * (1-p)/(2 * var * num_points)
else:
  w_decay = 0
optimizer = optim.Adam(dropout_mlp.parameters(), lr=1e-4, weight_decay=w_decay)
squared_loss = nn.MSELoss()

dropout_mlp.train()
for epoch in range(1, args.epochs + 1):
    dropout_mlp = train(dropout_mlp, x_data, y_data, optimizer, squared_loss)
    if epoch % 50 == 0:
        print('Training Dropout_MLP, epoch {}'.format(epoch))

n_test_samples = 1000
dropout_mlp.eval()
with torch.no_grad():
  y_dropout_mean = dropout_mlp(x_all)
  d1, d2 = y_dropout_mean.size()
  print(y_dropout_mean.size())
  y_dropout_samples = torch.zeros(n_test_samples, d1, d2)
  y_dropout_all = torch.zeros(n_test_samples+1, d1, d2)
  for i in range(n_test_samples):
    y_dropout_samples[i] = dropout_mlp(x_all)
    y_dropout_all[i] = y_dropout_samples[i]
  y_dropout_all[-1] = y_dropout_mean
  
  dropout_mean = torch.mean(y_dropout_all, dim=0)
  dropout_median = torch.median(y_dropout_all, dim=0)[0]

class GaussianReparam:
    def __init__(self, mu, rho):
        self.mu = mu
        self.rho = rho
        
    def sample(self):
        epsilon = torch.randn_like(self.rho) # "randn" samples from standard normal distr
        return self.mu + torch.log(1 + torch.exp(self.rho)) * epsilon
    
    def log_prob(self, x):
        mu = self.mu
        sigma = torch.log(1 + torch.exp(self.rho))
        return torch.distributions.normal.Normal(mu, sigma).log_prob(x).sum()
    
class ScaleMixtureGaussian:
    def __init__(self, pi, sigma_1, sigma_2):
        self.pi = pi
        self.gaussian_1 = torch.distributions.normal.Normal(0, sigma_1)
        self.gaussian_2 = torch.distributions.normal.Normal(0, sigma_2)
        
    def log_prob(self, x):
        prob1 = torch.exp(self.gaussian_1.log_prob(x)) # for some reason PyTorch doesn't have a pdf function so take exp(log_prob(.))
        prob2 = torch.exp(self.gaussian_2.log_prob(x))
        return torch.log(self.pi * prob1 + (1-self.pi) * prob2) # this will get summed across both dimensions

class BayesianLayer(nn.Module):
    """Creates a single layer i.e. weight matrix of a BNN"""
    def __init__(self, in_units, out_units, prior_form,
                 init_weights=(-0.2, 0.2, -5, -4, -0.2, 0.2, -5, -4)):  
        super().__init__()
        self.in_units = in_units
        self.out_units = out_units
        wma, wmb, wra, wrb, bma, bmb, bra, brb = init_weights
        # Weights (distribution to sample from)
        self.weight_mu = nn.Parameter(torch.Tensor(out_units, in_units).uniform_(wma, wmb)) # borrowed initalisation from nitarshan
        self.weight_rho = nn.Parameter(torch.Tensor(out_units, in_units).uniform_(wra, wrb))
        self.weight = GaussianReparam(self.weight_mu, self.weight_rho) # type: out_units x in_units matrix of GaussianReparam() objects (which we can sample from or find log_probs)
        # Biases (distribution to sample from)
        self.bias_mu = nn.Parameter(torch.Tensor(out_units).uniform_(bma, bmb))
        self.bias_rho = nn.Parameter(torch.Tensor(out_units).uniform_(bra, brb))
        self.bias = GaussianReparam(self.bias_mu, self.bias_rho)
        # Priors (_shared_ distribution to sample from)
        if len(prior_form) == 3:
            nl_sig1, nl_sig2, pi = prior_form
            self.weight_prior = ScaleMixtureGaussian(pi, math.exp(-nl_sig1), math.exp(-nl_sig2))
            self.bias_prior = ScaleMixtureGaussian(pi, math.exp(-nl_sig1), math.exp(-nl_sig2))
        else: # use standard guassian priors
            nl_sigma, = prior_form
            self.weight_prior = torch.distributions.normal.Normal(0, math.exp(-nl_sigma))
            self.bias_prior = torch.distributions.normal.Normal(0,  math.exp(-nl_sigma))
        # Initialise log probs of prior and variational posterior for this layer
        self.log_variational_posterior = 0
        self.log_prior = 0
        
    def forward(self, x, take_sample=True):
        """Do a forward pass by sampling from variational posterior
           Also update log probabilities for varational posterior and prior
           given the current sampled weights and biases
           (we will need theseSubsetRandomSampler to compute the loss function)
        """
        if take_sample or self.training: 
            weight = self.weight.sample()
            bias = self.bias.sample()
        else:
            weight = self.weight.mu
            bias = self.bias.mu
        if self.training: # (*)
            self.log_variational_posterior = self.weight.log_prob(weight).sum() + self.bias.log_prob(bias).sum()
            self.log_prior = self.weight_prior.log_prob(weight).sum() + self.bias_prior.log_prob(bias).sum()
        else:
            self.log_prior, self.log_variational_posterior = 0, 0
        return F.linear(x, weight, bias) # F.linear does not learn a bias by default, whereas nn.linear does

class BayesianNet(nn.Module):
    def __init__(self, in_units, hidden_units, out_units, args, bayes_params):
        super().__init__()
        n_train_samples, KL_reweight, prior_form = bayes_params
        self.v_samples = n_train_samples
        self.batch_size = args.batch_size
        self.num_batches = args.num_batches_train
        self.do_KL_reweighting = KL_reweight
        self.in_units = in_units
        self.out_units = out_units
        self.layer1 = BayesianLayer(in_units, hidden_units, prior_form,
                                    (-0.3, 0.3, -2.1, -2, -0.3, 0.3, -2.1, -2) )
#                                     (-1.5, 1.5, -5, -2, -1.5, 1.5, -5, -2) ) #(-1.27, 1.25, -1, -0.1, -1, 1, -2, -1) )
        self.layer2 = BayesianLayer(hidden_units, hidden_units, prior_form,
                                    (-0.3, 0.3, -2.1, -2, -0.3, 0.3, -2.1, -2) )
#                                     (0, 1, -5, -2, -0.1, 0.1, -5, -2) )
        self.layer3 = BayesianLayer(hidden_units, out_units, prior_form,
                                    (-0.3, 0.3, -2.1, -2, -0.3, 0.3, -2.1, -2) )
#                                     (-0.1, 0.1, -5, -2, -0.05, -0.04, -5, -2) )
#                                     (-0.1, 0.1, -5, -2, -0.05, -0.04, -5, -2) )
        self.squared_loss = nn.MSELoss(reduction='sum')
        
    def forward(self, x, take_sample=True): # note 5
        x = x.view(-1, self.in_units)
        x = F.relu(self.layer1(x, take_sample))
        x = F.relu(self.layer2(x, take_sample))
        x = self.layer3(x, take_sample)
        return x
    
    def log_prior(self):
        '''log probability of the current prior parameters is the sum
           of those parameters for each layer.
           These get updated here (*) each time we do a forward pass
           This implies forward() must be called before finding the log lik
           of the posterior and prior parameters (in the loss func)!
        '''
        return self.layer1.log_prior \
            + self.layer2.log_prior \
            + self.layer3.log_prior
    
    def log_variational_posterior(self):
        '''log probability of the current posterior parameters is the sum
            of those parameters for each layer
        '''
        return self.layer1.log_variational_posterior + \
               self.layer2.log_variational_posterior + \
               self.layer3.log_variational_posterior
      
    def loss(self, input, target, batch_idx, device):
        """Variational free energy/negative ELBO loss function, called
           f(w, theta) in the paper
           NB calling model.loss() does a forward pass, so in train() function
           we don't need to call model(input)
        """
        outputs = torch.zeros(self.v_samples, self.batch_size, self.out_units).to(device) # create tensors on the GPU
        outputs_x = torch.zeros(self.v_samples+1, self.batch_size, self.out_units).to(device)
        log_priors = torch.zeros(self.v_samples).to(device)
        log_variational_posteriors = torch.zeros(self.v_samples).to(device)
        for i in range(self.v_samples):
            outputs[i] = self(input) # note 4
            log_variational_posteriors[i] = self.log_variational_posterior()
            log_priors[i] = self.log_prior()
        log_variational_posterior = log_variational_posteriors.mean()
        log_prior = log_priors.mean()
        # TBH it would be better code if the rest of the following lines alone were the loss function, and the above was done in some other function (but not just 'forward')
#         squared_loss = nn.MSELoss(reduction='sum')
#         print('outputs', outputs.size())#         print('outputs.mean(0)', outputs.mean(0).size())
#         print('target', target.size())
        ###
        target_expanded = target.unsqueeze(0).repeat(self.v_samples,1,1)
        outputs_x[self.v_samples] = target
        for i in range(self.v_samples):
          outputs_x[i] = outputs[i]
        sigma = outputs_x.std(0).pow(2)
        sigma_expanded = sigma.unsqueeze(0).repeat(self.v_samples,1,1)
        negative_log_likelihood = (((outputs-target_expanded).pow(2)/(2*sigma_expanded)).mean(0) + 0.5*torch.log(2*math.pi*sigma)).sum()
        if self.do_KL_reweighting:
            minibatch_weight = 1. / (2**self.num_batches - 1) * (2**(self.num_batches - batch_idx))
            loss = minibatch_weight * (log_variational_posterior - log_prior) + negative_log_likelihood
        else:
            loss = 1/self.num_batches * (log_variational_posterior - log_prior) + negative_log_likelihood
#         print('log_variational_posterior: {:.2f}'.format(log_variational_posterior))
#         print('log_prior: {:.2f}'.format(log_prior))
        print('negative_log_likelihood: {:.2f}'.format(negative_log_likelihood))
        print('loss: ', loss.item())
        if outputs.sum() != outputs.sum():
          assert False
        return loss

# BayesianNet
variational_samples_train = 10
do_KL_reweight = False # only one batch so this is meaningless
nl_sigma1 = 0
nl_sigma2 = 6
pi = 0.25
spike_slab_prior = [nl_sigma1, nl_sigma2, pi]
gaussian_prior_nl_sigma = [3]
bayes_params = [variational_samples_train,
                do_KL_reweight, spike_slab_prior]

bayes_net = BayesianNet(1, 800, 1, args, bayes_params).to(device)
optimizer = optim.Adam(bayes_net.parameters(),lr=1e-3)

bayes_net.train()
for epoch in range(1, args.epochs + 1):
    bayes_net = train(bayes_net, x_cuda, y_cuda, optimizer, bayes_net.loss, device)
    if epoch % 50 == 0:
        print('Training BayesianNet, epoch {}'.format(epoch))

n_test_samples = 1000
bayes_net.eval()
with torch.no_grad():
    y_mean = bayes_net(x_all_cuda, take_sample=False)
    d1, d2 = y_mean.size()
    y_bayes_samples = torch.zeros(n_test_samples, d1, d2)
    y_bayes_all = torch.zeros(n_test_samples+1, d1, d2)
    for i in range(n_test_samples):
        y_bayes_samples[i] = bayes_net(x_all_cuda, take_sample=True)
        y_bayes_all[i] = y_bayes_samples[i]
    y_bayes_all[-1] = y_mean
    
    # compute mean and median
    mean = torch.mean(y_bayes_all, dim=0)
    median = torch.median(y_bayes_all, dim=0)[0] # torch.median also returns "argmedian"

plt.xlim(left=-0.4, right=1.2)
plt.ylim(top=1.2,bottom=-0.6)
plt.figure(1)
#mlp plot
for i in range(10):
  if i == 0:
    plt.scatter(x_all, y_MLP[i], marker=".", s=10, label='MLP', c='r')
  else:
    plt.scatter(x_all, y_MLP[i], marker=".", s=10, c='r')
#data plot
plt.scatter(x_data, y_data, marker=".", s=10, label='data', c='0') # pyplot can handle torch tensors, apparently
plt.legend()

# plt.figure(2)
# plt.xlim(left=-0.4, right=1.2)
# plt.ylim(top=1.2,bottom=-0.6)
# #bbb plot
# for i in range(n_test_samples):
#     plt.scatter(x_all, y_bayes_samples[i].cpu().numpy(), s=1, alpha=0.7, c='0.7')
# plt.scatter(x_all, y_mean.cpu().numpy(), marker=".", s=10, c='g', label='posterior mean')
# plt.scatter(x_all, mean.cpu().numpy(), marker=".", s=10, c='b', label='mean of all')
# plt.scatter(x_all, median.cpu().numpy(), marker=".", s=10, c='c', label='median of all')
# plt.scatter(x_data, y_data, marker=".", s=10, label='data', c='0') # pyplot can handle torch tensors, apparently
# plt.legend()

# plt.figure(3)
# plt.xlim(left=-0.4, right=1.2)
# plt.ylim(top=1.2,bottom=-0.6)
# #dropout plot
# for i in range(n_test_samples):
#     plt.scatter(x_all, y_dropout_samples[i].cpu().numpy(), s=1, alpha=0.7, c='0.7')
# plt.scatter(x_all, dropout_mean.cpu().numpy(), marker=".", s=10, c='b', label='dropout mean of all')
# plt.scatter(x_all, dropout_median.cpu().numpy(), marker=".", s=10, c='c', label='dropout median of all')
# plt.scatter(x_data, y_data, marker=".", s=10, label='data', c='0') # pyplot can handle torch tensors, apparently
# plt.legend()
# plt.show()